{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasagne\n",
    "* lasagne is a library for neural network building and training\n",
    "* it's a low-level library with almost seamless integration with theano\n",
    "\n",
    "For a demo we shall solve the same digit recognition problem, but at a different scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((50000, 1, 28, 28), (50000,))\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6a1f6c0390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8IivADQf0/sEWOix6VKakAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a57012c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "from lasagne.nonlinearities import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Input layer (auxilary)\n",
    "l1 = InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "l2 = ConvLayer(l1, 16, 5)\n",
    "\n",
    "l3 = PoolLayer(l2, 2)\n",
    "\n",
    "l4 = ConvLayer(l3, 32, 3)\n",
    "\n",
    "\n",
    "l5 = ConvLayer(l4, 32, 3)\n",
    "#\n",
    "\n",
    "l6 = DenseLayer(l5,num_units=100,nonlinearity = lasagne.nonlinearities.elu)\n",
    "\n",
    "\n",
    "l7 = DenseLayer(l6,num_units=150,nonlinearity = lasagne.nonlinearities.tanh)\n",
    "\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "l_out = DenseLayer(l7,num_units = 10,nonlinearity=lasagne.nonlinearities.softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(l_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b, W, b, W, b, W, b]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorType(float64, (False, True, False, False))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(l_out)\n",
    "print all_weights\n",
    "all_weights[0].type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Than you could simply\n",
    "* define loss function manually\n",
    "* compute error gradient over all weights\n",
    "* define updates\n",
    "* But that's a whole lot of work and life's short\n",
    "  * not to mention life's too short to wait for SGD to converge\n",
    "\n",
    "Instead, we shall use Lasagne builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adamax(loss, all_weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# inputs - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# outputs - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize):\n",
    "    assert len(inputs) == len(targets)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 72.395s\n",
      "  training loss (in-iteration):\t\t0.118316\n",
      "  train accuracy:\t\t96.31 %\n",
      "  validation accuracy:\t\t98.11 %\n",
      "Epoch 2 of 100 took 66.963s\n",
      "  training loss (in-iteration):\t\t0.046764\n",
      "  train accuracy:\t\t98.61 %\n",
      "  validation accuracy:\t\t98.53 %\n",
      "Epoch 3 of 100 took 67.062s\n",
      "  training loss (in-iteration):\t\t0.032276\n",
      "  train accuracy:\t\t99.03 %\n",
      "  validation accuracy:\t\t98.93 %\n",
      "Epoch 4 of 100 took 67.416s\n",
      "  training loss (in-iteration):\t\t0.022500\n",
      "  train accuracy:\t\t99.32 %\n",
      "  validation accuracy:\t\t98.91 %\n",
      "Epoch 5 of 100 took 67.437s\n",
      "  training loss (in-iteration):\t\t0.017945\n",
      "  train accuracy:\t\t99.45 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 6 of 100 took 67.262s\n",
      "  training loss (in-iteration):\t\t0.012264\n",
      "  train accuracy:\t\t99.64 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 7 of 100 took 67.842s\n",
      "  training loss (in-iteration):\t\t0.011346\n",
      "  train accuracy:\t\t99.63 %\n",
      "  validation accuracy:\t\t98.67 %\n",
      "Epoch 8 of 100 took 68.113s\n",
      "  training loss (in-iteration):\t\t0.007734\n",
      "  train accuracy:\t\t99.75 %\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 9 of 100 took 70.643s\n",
      "  training loss (in-iteration):\t\t0.007393\n",
      "  train accuracy:\t\t99.77 %\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 10 of 100 took 68.112s\n",
      "  training loss (in-iteration):\t\t0.005150\n",
      "  train accuracy:\t\t99.85 %\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 11 of 100 took 67.705s\n",
      "  training loss (in-iteration):\t\t0.004628\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t99.00 %\n",
      "Epoch 12 of 100 took 67.630s\n",
      "  training loss (in-iteration):\t\t0.004308\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t99.17 %\n",
      "Epoch 13 of 100 took 67.100s\n",
      "  training loss (in-iteration):\t\t0.003007\n",
      "  train accuracy:\t\t99.91 %\n",
      "  validation accuracy:\t\t99.09 %\n",
      "Epoch 14 of 100 took 67.304s\n",
      "  training loss (in-iteration):\t\t0.002688\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 15 of 100 took 66.825s\n",
      "  training loss (in-iteration):\t\t0.003047\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 16 of 100 took 67.893s\n",
      "  training loss (in-iteration):\t\t0.002667\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t99.15 %\n",
      "Epoch 17 of 100 took 66.798s\n",
      "  training loss (in-iteration):\t\t0.001961\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 18 of 100 took 66.754s\n",
      "  training loss (in-iteration):\t\t0.001627\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t99.15 %\n",
      "Epoch 19 of 100 took 66.891s\n",
      "  training loss (in-iteration):\t\t0.002454\n",
      "  train accuracy:\t\t99.92 %\n",
      "  validation accuracy:\t\t99.15 %\n",
      "Epoch 20 of 100 took 67.696s\n",
      "  training loss (in-iteration):\t\t0.001024\n",
      "  train accuracy:\t\t99.96 %\n",
      "  validation accuracy:\t\t99.14 %\n",
      "Epoch 21 of 100 took 68.827s\n",
      "  training loss (in-iteration):\t\t0.001507\n",
      "  train accuracy:\t\t99.96 %\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 22 of 100 took 67.108s\n",
      "  training loss (in-iteration):\t\t0.000689\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 23 of 100 took 66.927s\n",
      "  training loss (in-iteration):\t\t0.001506\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 24 of 100 took 69.052s\n",
      "  training loss (in-iteration):\t\t0.001253\n",
      "  train accuracy:\t\t99.96 %\n",
      "  validation accuracy:\t\t99.14 %\n",
      "Epoch 25 of 100 took 68.072s\n",
      "  training loss (in-iteration):\t\t0.002025\n",
      "  train accuracy:\t\t99.94 %\n",
      "  validation accuracy:\t\t99.22 %\n",
      "Epoch 26 of 100 took 67.518s\n",
      "  training loss (in-iteration):\t\t0.000730\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t99.17 %\n",
      "Epoch 27 of 100 took 67.082s\n",
      "  training loss (in-iteration):\t\t0.000885\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 28 of 100 took 67.078s\n",
      "  training loss (in-iteration):\t\t0.001502\n",
      "  train accuracy:\t\t99.96 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 29 of 100 took 67.001s\n",
      "  training loss (in-iteration):\t\t0.000323\n",
      "  train accuracy:\t\t99.99 %\n",
      "  validation accuracy:\t\t99.21 %\n",
      "Epoch 30 of 100 took 67.038s\n",
      "  training loss (in-iteration):\t\t0.000591\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 31 of 100 took 67.167s\n",
      "  training loss (in-iteration):\t\t0.001130\n",
      "  train accuracy:\t\t99.95 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 32 of 100 took 67.020s\n",
      "  training loss (in-iteration):\t\t0.001005\n",
      "  train accuracy:\t\t99.96 %\n",
      "  validation accuracy:\t\t99.21 %\n",
      "Epoch 33 of 100 took 66.997s\n",
      "  training loss (in-iteration):\t\t0.000743\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t99.15 %\n",
      "Epoch 34 of 100 took 66.937s\n",
      "  training loss (in-iteration):\t\t0.000868\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 35 of 100 took 67.137s\n",
      "  training loss (in-iteration):\t\t0.000417\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t99.29 %\n",
      "Epoch 36 of 100 took 67.197s\n",
      "  training loss (in-iteration):\t\t0.000790\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t99.26 %\n",
      "Epoch 37 of 100 took 66.904s\n",
      "  training loss (in-iteration):\t\t0.000821\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t99.09 %\n",
      "Epoch 38 of 100 took 66.908s\n",
      "  training loss (in-iteration):\t\t0.000829\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t99.20 %\n",
      "Epoch 39 of 100 took 67.108s\n",
      "  training loss (in-iteration):\t\t0.000106\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.16 %\n",
      "Epoch 40 of 100 took 67.210s\n",
      "  training loss (in-iteration):\t\t0.000460\n",
      "  train accuracy:\t\t99.99 %\n",
      "  validation accuracy:\t\t99.14 %\n",
      "Epoch 41 of 100 took 66.846s\n",
      "  training loss (in-iteration):\t\t0.000066\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.09 %\n",
      "Epoch 42 of 100 took 66.947s\n",
      "  training loss (in-iteration):\t\t0.000009\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.21 %\n",
      "Epoch 43 of 100 took 66.936s\n",
      "  training loss (in-iteration):\t\t0.000002\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 44 of 100 took 67.312s\n",
      "  training loss (in-iteration):\t\t0.000001\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.21 %\n",
      "Epoch 45 of 100 took 67.267s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.22 %\n",
      "Epoch 46 of 100 took 67.132s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.25 %\n",
      "Epoch 47 of 100 took 67.082s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.28 %\n",
      "Epoch 48 of 100 took 67.067s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.28 %\n",
      "Epoch 49 of 100 took 67.052s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.17 %\n",
      "Epoch 50 of 100 took 67.083s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 51 of 100 took 67.205s\n",
      "  training loss (in-iteration):\t\t0.000790\n",
      "  train accuracy:\t\t99.98 %\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 52 of 100 took 67.458s\n",
      "  training loss (in-iteration):\t\t0.000100\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.17 %\n",
      "Epoch 53 of 100 took 66.912s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.15 %\n",
      "Epoch 54 of 100 took 67.143s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.15 %\n",
      "Epoch 55 of 100 took 66.847s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.14 %\n",
      "Epoch 56 of 100 took 67.019s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.14 %\n",
      "Epoch 57 of 100 took 66.923s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.15 %\n",
      "Epoch 58 of 100 took 67.190s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.18 %\n",
      "Epoch 59 of 100 took 67.302s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.19 %\n",
      "Epoch 60 of 100 took 66.997s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.21 %\n",
      "Epoch 61 of 100 took 67.010s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.20 %\n",
      "Epoch 62 of 100 took 67.420s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.22 %\n",
      "Epoch 63 of 100 took 67.006s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.23 %\n",
      "Epoch 64 of 100 took 67.399s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.23 %\n",
      "Epoch 65 of 100 took 67.629s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.22 %\n",
      "Epoch 66 of 100 took 67.172s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.22 %\n",
      "Epoch 67 of 100 took 67.453s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.24 %\n",
      "Epoch 68 of 100 took 68.543s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.23 %\n",
      "Epoch 69 of 100 took 67.893s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.25 %\n",
      "Epoch 70 of 100 took 67.971s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.26 %\n",
      "Epoch 71 of 100 took 67.994s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.25 %\n",
      "Epoch 72 of 100 took 67.423s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.25 %\n",
      "Epoch 73 of 100 took 67.573s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.27 %\n",
      "Epoch 74 of 100 took 66.993s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.26 %\n",
      "Epoch 75 of 100 took 68.566s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.25 %\n",
      "Epoch 76 of 100 took 67.473s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.27 %\n",
      "Epoch 77 of 100 took 67.554s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.26 %\n",
      "Epoch 78 of 100 took 67.776s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.24 %\n",
      "Epoch 79 of 100 took 67.414s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.26 %\n",
      "Epoch 80 of 100 took 67.142s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.27 %\n",
      "Epoch 81 of 100 took 68.648s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.26 %\n",
      "Epoch 82 of 100 took 68.314s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.27 %\n",
      "Epoch 83 of 100 took 67.120s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.26 %\n",
      "Epoch 84 of 100 took 68.062s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.28 %\n",
      "Epoch 85 of 100 took 67.826s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.28 %\n",
      "Epoch 86 of 100 took 67.492s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.28 %\n",
      "Epoch 87 of 100 took 67.562s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.29 %\n",
      "Epoch 88 of 100 took 67.869s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.28 %\n",
      "Epoch 89 of 100 took 69.428s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.28 %\n",
      "Epoch 90 of 100 took 73.287s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.29 %\n",
      "Epoch 91 of 100 took 67.003s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.29 %\n",
      "Epoch 92 of 100 took 66.793s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.29 %\n",
      "Epoch 93 of 100 took 66.837s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.29 %\n",
      "Epoch 94 of 100 took 66.919s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.30 %\n",
      "Epoch 95 of 100 took 66.914s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.30 %\n",
      "Epoch 96 of 100 took 66.790s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.30 %\n",
      "Epoch 97 of 100 took 67.110s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.30 %\n",
      "Epoch 98 of 100 took 68.494s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.30 %\n",
      "Epoch 99 of 100 took 66.851s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.30 %\n",
      "Epoch 100 of 100 took 66.793s\n",
      "  training loss (in-iteration):\t\t0.000000\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t99.30 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 #amount of passes through the data\n",
    "\n",
    "batch_size = 50 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t99.30 %\n",
      "Achievement unlocked: 80lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now improve it!\n",
    "\n",
    "* Moar layers!\n",
    "* Moar units!\n",
    "* Different nonlinearities!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
